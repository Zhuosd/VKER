# Exploring Voice- and Knowledge-aware for Enhanced Recommendation Systems
## Abstract
Recommendation systems, as a critical technology in information filtering, play a pivotal role in helping users discover preferred content from vast information repositories by modeling individual interaction behaviors. However, existing approaches predominantly focus on text-based interactions (e.g., user-item), often neglecting the complementary and latent signals embedded in other modalities, such as voice interactions and the structural knowledge represented by knowledge graphs. Voice data, in particular, encapsulates rich semantic cues about user preferences, while knowledge graphs provide structured contextual insights that can enhance the recommendation process. Motivated by the potential of these underutilized modalities, this paper introduces a novel framework, named **VKER**, which integrates voice interaction and knowledge graph features to enhance recommendation systems. The proposed framework establishes semantic pathways across text, voice, and knowledge graph features to align heterogeneous representations via user-item interaction retrieval. Furthermore, graph-based aggregation strategies are employed to smooth multimodal representations and capture intricate user-voice- item interaction patterns. Extensive experiments demonstrate the effectiveness of VKER in outperforming traditional text-based models, showcasing its ability to leverage voice and knowledge graph data for superior recommendation performance.